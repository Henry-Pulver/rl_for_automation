import sys
# sys.path.append("/home/ec2-user/SageMaker/rl_for_automation")
# !pwd
sys.path
!{sys.executable} -m pip install gym
!{sys.executable} -m pip install atari_py

import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

from algorithms.PPO import PPO, hyperparams_ppo_atari, main
import datetime
from algorithms.PPO import HyperparametersPPO, train_ppo
env_name = "Breakout-ram-v4"
solved_reward = 300  # stop training if avg_reward > solved_reward
log_interval = 1  # print avg reward in the interval
max_episodes = 1000000  # max training episodes
max_timesteps = None  # max timesteps in one episode
update_timestep = 1024
random_seeds = list(range(0, 5))
actor_layers = (128, 128)
actor_activation = "relu"
critic_layers = (128, 128)
critic_activation = "relu"
ppo_type = "clip"
date = datetime.date.today().strftime("%d-%m-%Y")
outcomes = []

try:
    outcomes.append(env_name)
    hyp = HyperparametersPPO(
        gamma=0.99,  # discount factor
        lamda=0.95,
        learning_rate=2e-3,
        T=1024,  # update policy every n timesteps
        epsilon=0.2,  # clip parameter for PPO
        c2=0.01,
        num_epochs=3,  # update policy for K epochs
    )

    outcomes.append(
        train_ppo(
            env_name=env_name,
            solved_reward=solved_reward,
            hyp=hyp,
            random_seeds=random_seeds,
            actor_layers=actor_layers,
            actor_activation=actor_activation,
            critic_layers=critic_layers,
            critic_activation=critic_activation,
            update_timestep=update_timestep,
            log_interval=log_interval,
            max_episodes=max_episodes,
            max_timesteps=max_timesteps,
            ppo_type=ppo_type,
            verbose=False,
            date=date,
            render=False,
        )
    )
finally:
    print(f"outcomes:")
    for outcome in outcomes:
        print(outcome)